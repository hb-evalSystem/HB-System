# ğŸ§  HB-Eval Systemâ„¢ â€” Open-Core Edition

**The First Comprehensive Behavioral Evaluation Framework for Agentic AI**
<div align="center">
  <img src="assets/hb-eval-logo.png" alt="HB-Eval Logo" width="500"/>
  
  <h1>HB-Eval: Hybrid Behavioral Evaluation Framework</h1>
  
  <p><strong>A comprehensive evaluation framework for embodied AI agents</strong></p>

  [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
  [![Python](https://img.shields.io/badge/Python-3.8%2B-green.svg)](https://www.python.org/)
  [![arXiv](https://img.shields.io/badge/arXiv-Coming%20Soon-b31b1b.svg)](#research)

</div>

---

## ğŸ¯ What is HB-Eval System?

**HB-Eval System** is the **first comprehensive behavioral evaluation framework** for Agentic AI, addressing critical gaps in reliability, transparency, and trustworthiness through a **4-paper research series** and novel evaluation metrics.

### ğŸŒŸ Core Innovation

Unlike outcome-focused benchmarks (AgentBench, GAIA), HB-Eval introduces **9 novel process-based behavioral metrics**:

| Metric | Measures | Paper | Typical Value |
|--------|----------|-------|---------------|
| **FRR** | Failure Recovery Rate | Paper 1 | 92-100% |
| **PEI** | Planning Efficiency Index | Papers 1,2,3 | 0.90-0.92 |
| **TI** | Traceability Index (Transparency) | Papers 1,4 | 4.5-4.8/5.0 |
| **MIR** | Memory Immunization Rate | Paper 1 | 85-90% |
| **MP** | Memory Precision | Paper 3 | 88.4% |
| **MRS** | Memory Retention Stability | Paper 3 | 0.07 |
| **CER** | Cognitive Efficiency Ratio | Paper 3 | 0.73 |
| **Î”PEIâˆ** | Cumulative Learning | Paper 3 | +0.21 |
| **UAS** | Unified Agent Score | Paper 1 | 0.87 |

### ğŸ† Research Validation

- **4-Paper Series**: Comprehensive coverage from evaluation to human trust
- **500-Task Longitudinal Study**: First system showing positive cumulative learning (Î”PEIâˆ = +0.21)
- **Human Study (n=240)**: Trust Score = **4.62/5.0** â€” **highest ever reported** in agentic AI
- **AP-EDM Agent**: Achieves **FRR=100%**, **PEI=0.92**, surpassing ReAct & Reflexion

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/hb-evalSystem/HB-System.git
cd HB-System

# Install (choose one method)
pip install -e .                    # Editable install
pip install -r requirements.txt     # Requirements only
```

### Basic Usage (3 Lines!)

```python
from hb_eval import EDM, AdaptPlan, AgentLoop

agent = AgentLoop(EDM(), AdaptPlan())
result = agent.run("Optimize system performance")
print(f"PEI: {result.metrics.pei:.2f}")  # e.g., PEI: 0.92
```

### Run Interactive Demo

```bash
python -m hb_eval.demo
# or
python hb_eval/demo.py
```

---

## ğŸ“Š Why HB-Eval? (Comparison)

### vs Traditional Benchmarks

| Framework | Focus | Metrics | HB-Eval Integration |
|-----------|-------|---------|---------------------|
| **AgentBench** | Task completion | Success Rate | âœ… Add behavioral depth |
| **GAIA** | Multi-modal tasks | Accuracy | âœ… Measure reliability |
| **AutoGenBench** | AutoGen agents | Speed, Cost | âœ… Evaluate planning |
| **HB-Eval** | **Behavioral reliability** | **9 novel metrics** | â€” Native framework |

### vs Agent Building Frameworks

| Framework | Type | Purpose | Relationship to HB-Eval |
|-----------|------|---------|-------------------------|
| **LangChain** | Builder | Construct agents | âœ… **Build** with LangChain â†’ **Evaluate** with HB-Eval |
| **AutoGen** | Orchestrator | Multi-agent systems | âœ… **Orchestrate** with AutoGen â†’ **Validate** with HB-Eval |
| **CrewAI** | Coordinator | Team collaboration | âœ… **Coordinate** with CrewAI â†’ **Monitor** with HB-Eval |

**Use Case Flow**: `Build Agent (LangChain) â†’ Evaluate (HB-Eval) â†’ Deploy with Confidence`

---

## ğŸ“š Research Foundation

### Four-Paper Research Series

This framework is backed by comprehensive research addressing **Four Critical Gaps**:

```
Gap 1: Evaluation Crisis
    â†“ Paper 1: HB-Eval Framework
Gap 2: Adaptation & Reasoning  
    â†“ Paper 2: Adapt-Plan Architecture
Gap 3: Long-Term Memory
    â†“ Paper 3: Eval-Driven Memory (EDM)
Gap 4: Trust & Transparency
    â†“ Paper 4: HCI-EDM (Human Trust)
```

### Key Results (From Papers)

#### Paper 1: Evaluation Framework

| Agent | SR | FRR | PEI | TI | UAS |
|-------|-----|-----|-----|----|----|
| ReAct | 85% | 40% | 0.75 | 4.5 | 0.65 |
| Reflexion | 82% | 75% | 0.60 | 3.2 | 0.72 |
| **AP-EDM** | **88%** | **100%** | **0.90** | **4.8** | **0.87** |

**Key Finding**: UAS ranking aligns perfectly with human evaluation (Spearman Ï=1.00)

#### Paper 3: Memory System (500-Task Study)

| System | MP | MRS | CER | Î”PEIâˆ | Final PEI |
|--------|-----|-----|-----|-------|-----------|
| Flat Memory | 47% | 0.24 | 1.04 | **-0.19** âŒ | 0.61 |
| Recency-Only | 62% | 0.18 | 0.91 | **-0.08** âŒ | 0.70 |
| Generative Agents | 69% | 0.15 | 0.87 | +0.03 | 0.79 |
| **EDM** | **88.4%** | **0.07** | **0.73** | **+0.21** âœ… | **0.92** |

**Key Finding**: EDM is the **only system** showing positive cumulative learning

#### Paper 4: Human Trust Study (n=240)

| Metric | CoT Baseline | HCI-EDM | Improvement |
|--------|-------------|---------|-------------|
| Trust Score | 3.10 | **4.62** | **+49%** |
| Transparency | 0.45 | **0.91** | **+102%** |
| Cognitive Load | 18.5s | **9.2s** | **-51%** |
| Error Detection | 65% | **90%** | **+38%** |

**Key Finding**: 4.62/5.0 is the **highest trust score ever reported** in agentic AI

ğŸ“„ **Full Research Documentation**: See [RESEARCH.md](RESEARCH.md)

---

## ğŸ§© Core Components

### 1. Eval-Driven Memory (EDM)

```python
from hb_eval import EDM, Experience, ExperienceMetrics, Plan

# Initialize with quality threshold
edm = EDM(storage_threshold=0.78)

# Store high-quality experience
plan = Plan(goal="Optimize", sub_goals=["Analyze", "Execute"])
exp = Experience(plan=plan, metrics=ExperienceMetrics(pei=0.92))
edm.store(exp)  # Stores only if PEI â‰¥ 0.78

# Retrieve similar experience
retrieved = edm.retrieve_procedural_guide("Optimize system")
```

**Features**: Selective consolidation â€¢ Semantic retrieval â€¢ Performance metadata â€¢ Cumulative learning

### 2. Adaptive Planner (Adapt-Plan)

```python
from hb_eval import AdaptPlan

planner = AdaptPlan(enable_verbose=True)

# Generate plan (retrieves from memory if available)
plan = planner.generate_plan("Deploy new feature", edm)

# PEI-guided control: adapts if PEI < 0.70
# Strategic replanning vs Tactical adaptation
```

**Features**: PEI-guided control â€¢ Dual planning â€¢ Semantic generalization â€¢ Fault recovery

### 3. Agent Execution Loop

```python
from hb_eval import AgentLoop

agent = AgentLoop(
    edm=edm,
    planner=planner,
    max_recovery_attempts=3,
    enable_verbose=True
)

# Execute with automatic metrics
result = agent.run("Complex task", store_experience=True)

# Access metrics
print(f"Steps: {len(result.plan.steps_taken)}")
print(f"PEI: {result.metrics.pei:.2f}")
print(f"Status: {result.status}")
```

**Features**: Step-by-step execution â€¢ Real-time PEI â€¢ Automatic recovery â€¢ Experience storage

### 4. LLM Integration

```python
from hb_eval.core.external_llm_api import LLMConfig, LLMProvider, set_global_config

# OpenAI
config = LLMConfig(
    provider=LLMProvider.OPENAI,
    api_key="your-key",
    model="gpt-4"
)
set_global_config(config)

# Mock mode (testing)
mock_config = LLMConfig(provider=LLMProvider.MOCK)
set_global_config(mock_config)
```

**Supported**: OpenAI â€¢ Mock mode â€¢ Custom endpoints â€¢ Retry logic

---

## ğŸ“¦ Project Structure

```
HB-System/
â”œâ”€â”€ hb_eval/              # Main package â­
â”‚   â”œâ”€â”€ core/            # Core modules (EDM, Adapt-Plan, Agent Loop)
â”‚   â”œâ”€â”€ utils/           # Utilities
â”‚   â””â”€â”€ demo.py          # Interactive demo
â”œâ”€â”€ tests/               # Test suite (>80% coverage)
â”œâ”€â”€ examples/            # Usage examples
â”œâ”€â”€ papers/              # Research papers summaries ğŸ“„
â”œâ”€â”€ docs/                # Documentation
â”‚   â””â”€â”€ metrics.md       # Complete metrics guide
â”œâ”€â”€ benchmarks/          # Benchmarking suite ğŸ§ª
â”‚   â”œâ”€â”€ datasets/        # 500-task benchmark
â”‚   â”œâ”€â”€ baselines/       # Reference implementations
â”‚   â””â”€â”€ fit/             # Fault Injection Testbed
â”œâ”€â”€ tasks/               # Task definitions
â”œâ”€â”€ RESEARCH.md          # Research summary
â”œâ”€â”€ ROADMAP.md           # Development roadmap
â”œâ”€â”€ CITATION.bib         # Citation file
â”œâ”€â”€ CHANGELOG.md         # Version history
â””â”€â”€ README.md            # This file
```

---

## ğŸ³ Docker Support

```bash
# Build
docker build -t hb-eval-system:latest .

# Run demo
docker run --rm hb-eval-system:latest

# Interactive mode
docker run --rm -it hb-eval-system:latest python -m hb_eval.demo

# With API key
docker run --rm -e LLM_API_KEY="your-key" hb-eval-system:latest
```

**Image**: Multi-stage build â€¢ Non-root user â€¢ Health checks â€¢ ~85MB

---

## ğŸ§ª Testing & Benchmarking

### Run Tests

```bash
# All tests
pytest

# With coverage
pytest --cov=hb_eval --cov-report=html

# Specific module
pytest tests/test_core.py -v
```

### Run Benchmarks

```bash
# Quick benchmark (50 tasks)
python benchmarks/quick_benchmark.py --agent your_agent

# Full benchmark (500 tasks)
python benchmarks/run_benchmark.py --dataset core_500

# Compare with baselines
python benchmarks/compare_agents.py --agents react reflexion your_agent
```

ğŸ“Š **Benchmarking Guide**: See [benchmarks/README.md](benchmarks/README.md)

---

## ğŸ“ˆ Metrics Reference

### Core Behavioral Metrics

| Metric | Formula | Range | Ideal | Use Case |
|--------|---------|-------|-------|----------|
| **FRR** | Îµ_recovered / Îµ_total | 0-100% | â‰¥80% | Reliability |
| **PEI** | L_min / L_actual | 0.0-1.0 | â‰¥0.80 | Efficiency |
| **TI** | Avg(Judge scores) | 1.0-5.0 | â‰¥4.0 | Transparency |
| **MIR** | Correct / Total queries | 0.0-1.0 | â‰¥0.85 | Security |

### Memory Metrics (Paper 3)

| Metric | Measures | Lower/Higher Better |
|--------|----------|---------------------|
| **MP** | Quality of retrieval | Higher (88.4%) |
| **MRS** | Performance stability | Lower (0.07) |
| **CER** | Reasoning efficiency | Lower (<1.0) |
| **Î”PEIâˆ** | Cumulative learning | Higher (+0.21) |

ğŸ“– **Complete Guide**: See [docs/metrics.md](docs/metrics.md)

---

## ğŸ“ Citation

If you use HB-Eval System in your research:

```bibtex
@software{hb_eval_system_2025,
  title = {{HB-Eval System: Behavioral Evaluation \& Trustworthy Agentic AI}},
  author = {Abuelgasim, A.},
  year = {2025},
  version = {1.0.0-alpha},
  url = {https://github.com/hb-evalSystem/HB-System},
  license = {Apache-2.0}
}
```

**Full Citations**: See [CITATION.bib](CITATION.bib)

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Areas of Interest

- ğŸ› Bug reports & fixes
- ğŸ“Š Independent benchmark validation
- ğŸ”Œ New LLM provider integrations
- ğŸ“ Documentation improvements
- ğŸ§ª New evaluation metrics
- ğŸ’¡ Feature suggestions

**Special Call**: We encourage **independent validation** of our reported metrics!

---

## ğŸ—ºï¸ Roadmap

### Phase 1 (0-6 months) - Current

- âœ… Open-source launch
- ğŸ”„ Documentation site
- ğŸ”„ ArXiv pre-prints (Jan 2026)
- ğŸ”„ Conference submissions
- ğŸ¯ 500+ GitHub stars
- ğŸ¯ 10+ contributors

### Phase 2 (6-12 months)

- ğŸ“Š Public benchmark dataset
- ğŸ† Online leaderboard
- ğŸ¤ Academic partnerships
- ğŸ’¼ Industry pilots
- ğŸŒ Community growth

ğŸ“… **Full Roadmap**: See [ROADMAP.md](ROADMAP.md)

---

## ğŸ“œ License

**Open-Core**: Apache License 2.0

### You May:
- âœ… Use for research & academic work
- âœ… Use for commercial applications (open-core components)
- âœ… Modify and create derivatives
- âœ… Distribute and sublicense

### You Must:
- ğŸ“‹ Include original license notice
- ğŸ“‹ State significant changes
- ğŸ“‹ Include copy of Apache 2.0 license

### Commercial License

**Enterprise features** (MetaController, Advanced EDM) require separate commercial license.

ğŸ“„ See [COMMERCIAL_LICENSE.md](COMMERCIAL_LICENSE.md) for details.

---

## ğŸ“ Contact & Support

### General
- **Email**: hbevalframe@gmail.com
- **GitHub Issues**: [Report bugs](https://github.com/hb-evalSystem/HB-System/issues)
- **Discussions**: [Ask questions](https://github.com/hb-evalSystem/HB-System/discussions)

### Research Collaboration
- Independent validation
- Joint projects
- Academic partnerships
- Industry pilots

### Commercial
- Enterprise features
- Custom evaluation frameworks
- Training & consulting

**Response Time**: Usually within 48 hours

---

## ğŸŒŸ Acknowledgments

### Research Inspiration
- ReAct (Yao et al., 2022)
- Reflexion (Shinn et al., 2023)
- Generative Agents (Park et al., 2023)

### Community
- Open-source AI community
- Early testers and contributors
- Academic researchers providing feedback

---

## ğŸ“Š Project Status

| Aspect | Status | Notes |
|--------|--------|-------|
| **Code** | âœ… Stable | v1.0.0-alpha released |
| **Research** | ğŸ”„ Under Review | Papers submitted Q1 2026 |
| **Documentation** | âœ… Complete | Comprehensive guides |
| **Testing** | âœ… >80% Coverage | CI/CD automated |
| **Benchmarks** | ğŸ”„ In Progress | Public release Q1 2026 |
| **Community** | ğŸŒ± Growing | Just launched |

---

## ğŸ”— Links

- ğŸ  **Homepage**: https://github.com/hb-evalSystem/HB-System
- ğŸ“š **Documentation**: [README](README.md) â€¢ [Research](RESEARCH.md) â€¢ [Metrics](docs/metrics.md)
- ğŸ—ºï¸ **Roadmap**: [ROADMAP.md](ROADMAP.md)
- ğŸ“„ **Papers**: [papers/README.md](papers/README.md)
- ğŸ§ª **Benchmarks**: [benchmarks/README.md](benchmarks/README.md)
- ğŸ“– **Changelog**: [CHANGELOG.md](CHANGELOG.md)

---

## ğŸ¯ Why Choose HB-Eval?

âœ… **First of its kind** - Only comprehensive behavioral evaluation framework  
âœ… **Research-backed** - 4-paper series with empirical validation  
âœ… **Proven results** - Highest human trust score (4.62/5.0) ever reported  
âœ… **Production-ready** - Clean code, tests, CI/CD  
âœ… **Well-documented** - Extensive guides and examples  
âœ… **Open & extensible** - Apache 2.0, community-driven  
âœ… **Actively maintained** - Regular updates and support  

---

<p align="center">
  <b>ğŸš€ Start evaluating trustworthy agents today!</b><br>
  <code>pip install -e .</code> Â· <code>python -m hb_eval.demo</code>
</p>

<p align="center">
  <i>Built with â¤ï¸ for the AI Research Community</i><br>
  <i>Â© 2025 Abuelgasim Mohamed Ibrahim Adam. All rights reserved.</i>
</p>

---

**â­ If you find this useful, please star the repository!**
