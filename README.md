# ğŸ§  HB-Eval Systemâ„¢ â€” Open-Core Edition

**The Leading Behavioral Evaluation & Trustworthy Agentic AI Framework**

<p align="center">
  <img src="https://img.shields.io/badge/PEI-0.92-8A2BE2?style=for-the-badge" alt="PEI Score" />
  <img src="https://img.shields.io/badge/FRR-92%25-32CD32?style=for-the-badge" alt="FRR Score" />
  <img src="https://img.shields.io/badge/Human%20Trust-4.62%2F5.0-1E90FF?style=for-the-badge" alt="Trust Score" />
  <img src="https://img.shields.io/badge/Benchmark-500%20Tasks-orange?style=for-the-badge" alt="Benchmark" />
  <img src="https://img.shields.io/badge/Series-4%20Papers-red?style=for-the-badge" alt="Papers" />
  <img src="https://img.shields.io/badge/License-Apache%202.0-green?style=for-the-badge" alt="License" />
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge" alt="Python Version" />
</p>

---

## ğŸ“– Overview

**HB-Eval System** is a research-grade framework for **behavioral evaluation** and **trustworthy agentic AI**. This Open-Core edition provides the essential components for evaluating, planning, and executing AI agent behaviors with measurable performance metrics.

### ğŸ¯ Key Capabilities

| Component | Description | Status |
|-----------|-------------|--------|
| **PEI/FRR/TI Metrics** | Behavioral performance evaluation | âœ… Open-Core |
| **Adaptive Planning** | Rule-based deterministic planner | âœ… Open-Core |
| **Eval-Driven Memory (EDM)** | Experience-based plan retrieval | âœ… Open-Core |
| **LLM Integration** | Pluggable external LLM support | âœ… Open-Core |
| **Agent Loop** | Step-by-step execution with recovery | âœ… Open-Core |
| **MetaController (XAI)** | Advanced explainability layer | ğŸ”’ Commercial |
| **Semantic EDM** | Embedding-based memory | ğŸ”’ Commercial |

### ğŸš¨ Important Note on Metrics

The performance metrics shown in badges (PEI=0.92, FRR=92%, Trust=4.62/5.0) are results from **internal testing**. This Open-Core release is designed to facilitate **independent verification** and **external benchmarking** by the research community.

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/hb-evalSystem/HB-System.git
cd HB-System

# Install dependencies
pip install -e .

# Or install from requirements
pip install -r requirements.txt
```

### Basic Usage

```python
from hb_eval import EDM, AdaptPlan, AgentLoop

# Initialize components
edm = EDM()
planner = AdaptPlan()
agent = AgentLoop(edm, planner)

# Execute a goal
result = agent.run("Optimize system performance")
print(result)
```

### Run the Demo

```bash
# Interactive demo with examples
python -m hb_eval.demo

# Or run directly
python hb_eval/demo.py
```

---

## ğŸ“¦ Project Structure

```
HB-System/
â”œâ”€â”€ hb_eval/                    # Main package
â”‚   â”œâ”€â”€ __init__.py            # Package exports
â”‚   â”œâ”€â”€ core/                  # Core modules
â”‚   â”‚   â”œâ”€â”€ edm_memory.py      # Eval-Driven Memory
â”‚   â”‚   â”œâ”€â”€ adapt_planner.py   # Adaptive Planner
â”‚   â”‚   â”œâ”€â”€ agent_loop.py      # Agent Execution Loop
â”‚   â”‚   â””â”€â”€ external_llm_api.py # LLM API wrapper
â”‚   â”œâ”€â”€ utils/                 # Utilities
â”‚   â””â”€â”€ demo.py                # Interactive demo
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ papers/                    # Research papers (4-paper series)
â”œâ”€â”€ tasks/                     # Task definitions (500+ tasks)
â”œâ”€â”€ .github/workflows/         # CI/CD automation
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ examples/                  # Usage examples
â”œâ”€â”€ setup.py                   # Setup script
â”œâ”€â”€ pyproject.toml             # Modern Python config
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ Dockerfile                 # Container image
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ LICENSE                    # Apache 2.0 License
```

---

## ğŸ§© Core Components

### 1. Eval-Driven Memory (EDM)

EDM stores and retrieves past planning experiences based on similarity and performance:

```python
from hb_eval import EDM, Experience, ExperienceMetrics, Plan

# Initialize EDM
edm = EDM(storage_threshold=0.75, retrieval_threshold=0.40)

# Create and store an experience
plan = Plan(goal="Optimize workflow", sub_goals=["Step 1", "Step 2"])
exp = Experience(plan=plan, metrics=ExperienceMetrics(pei=0.85))
edm.store(exp)

# Retrieve similar experience
retrieved = edm.retrieve_procedural_guide("Optimize processes")
```

**Key Features:**
- Jaccard similarity matching
- PEI-based quality filtering
- Memory size management
- Top-N retrieval by performance

### 2. Adaptive Planner

Generates procedural plans through retrieval or template-based generation:

```python
from hb_eval import AdaptPlan, EDM

planner = AdaptPlan(enable_verbose=True)
edm = EDM()

# Generate plan (retrieves from memory if available)
plan = planner.generate_plan("Improve efficiency", edm)

# Force new plan generation
new_plan = planner.generate_plan("New goal", edm, force_new=True)

# Recovery replanning
recovery_plan = planner.replan(failed_plan, edm, failure_point=2)
```

**Planning Strategies:**
- Memory-based retrieval
- Template-based generation
- Adaptive replanning
- Failure recovery

### 3. Agent Execution Loop

Step-by-step plan execution with metrics tracking:

```python
from hb_eval import AgentLoop, EDM, AdaptPlan

edm = EDM()
planner = AdaptPlan()
agent = AgentLoop(
    edm, 
    planner, 
    max_recovery_attempts=3,
    enable_verbose=True
)

# Execute with automatic experience storage
result = agent.run("Complete task", store_experience=True)
```

**Features:**
- Step-by-step execution
- Real-time metrics (PEI, FRR, TI)
- Automatic failure recovery
- Experience storage
- Execution callbacks

### 4. LLM Integration

Flexible LLM API integration with multiple providers:

```python
from hb_eval.core.external_llm_api import (
    LLMConfig, LLMProvider, set_global_config, llm_call
)

# Configure for OpenAI
config = LLMConfig(
    provider=LLMProvider.OPENAI,
    api_key="your-key",
    model="gpt-3.5-turbo",
    temperature=0.7
)
set_global_config(config)

# Make calls
response = llm_call("Your prompt here")

# Mock mode for testing
mock_config = LLMConfig(provider=LLMProvider.MOCK)
set_global_config(mock_config)
```

**Supported Modes:**
- OpenAI API
- Mock mode (testing)
- Custom endpoints
- Automatic retry logic
- Error handling

---

## ğŸ³ Docker Support

### Build Image

```bash
docker build -t hb-eval-system:latest .
```

### Run Demo

```bash
docker run --rm hb-eval-system:latest
```

### Interactive Mode

```bash
docker run --rm -it hb-eval-system:latest python -m hb_eval.demo
```

### With API Key

```bash
docker run --rm -e LLM_API_KEY="your-key" hb-eval-system:latest
```

---

## ğŸ“Š Performance Metrics

### PEI (Performance Efficiency Index)

Measures overall execution efficiency:
- **Formula**: `completion_rate - failure_penalty - recovery_penalty`
- **Range**: 0.0 to 1.0
- **Threshold**: â‰¥0.75 for storage

### FRR (Failure Recovery Rate)

Measures resilience and recovery capability:
- **Calculation**: Success rate after failures
- **Range**: 0.0 to 1.0 (or 0% to 100%)
- **Target**: â‰¥90% for production systems

### TI (Task Integrity)

Measures correctness and goal alignment:
- **Evaluation**: Goal achievement verification
- **Range**: 0.0 to 1.0
- **Minimum**: â‰¥0.80 for acceptable results

---

## ğŸ§ª Testing & Development

### Run Tests

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest

# With coverage
pytest --cov=hb_eval --cov-report=html
```

### Code Quality

```bash
# Format code
black hb_eval/

# Sort imports
isort hb_eval/

# Type checking
mypy hb_eval/

# Linting
flake8 hb_eval/
```

---

## ğŸ“š Research Papers

The **HB-Eval System** is backed by a 4-paper research series (2025):

1. **Paper 1**: Behavioral Gaps in Current Agentic AI Systems
2. **Paper 2**: Adaptive Planning Under Uncertainty
3. **Paper 3**: Memory-Grounded Behavior Correction
4. **Paper 4**: Human Trust & Multi-Agent Alignment

> ğŸ“„ Upon official publication, all papers will include detailed experimental protocols, complete task datasets, and human subject descriptions for full reproducibility.

**Current Status**: Papers in preparation for submission to top-tier AI conferences.

---

## ğŸ“– Documentation

### API Reference

Full API documentation is available in the `docs/` directory:
- Core modules documentation
- Usage examples
- Best practices
- Troubleshooting guide

### Examples

Check the `examples/` directory for:
- Basic usage patterns
- Advanced configurations
- Integration examples
- Custom extensions

---

## ğŸ¤ Contributing

We welcome contributions from the research community!

### How to Contribute

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Areas of Interest

- ğŸ› Bug fixes and issue reports
- ğŸ“ˆ Performance improvements
- ğŸ§ª New evaluation metrics
- ğŸ”§ Additional planning algorithms
- ğŸ“Š Benchmark validations
- ğŸ“ Documentation improvements
- ğŸ¨ UI/UX enhancements

---

## ğŸ’¼ Commercial Version

The **HB-Eval System Premium-Core** includes:

| Feature | Open-Core | Premium-Core |
|---------|-----------|--------------|
| PEI/FRR/TI Evaluation | âœ… | âœ… |
| Basic EDM | âœ… | âœ… |
| Adaptive Planning | âœ… | âœ… |
| LLM Integration | âœ… | âœ… |
| **MetaController (XAI)** | âŒ | âœ… |
| **Semantic EDM** | âŒ | âœ… |
| **Real-time Monitoring** | âŒ | âœ… |
| **Multi-Agent Coordination** | âŒ | âœ… |
| **Enterprise Support** | âŒ | âœ… |

**ğŸš§ Commercial Launch Status**: Currently under development. For licensing inquiries, please contact us via email or check the company website upon launch.

---

## ğŸ“œ License

This Open-Core version is licensed under **Apache License 2.0**.

### You may:
- âœ… Use for research and academic work
- âœ… Use for commercial integration (Open-Core components only)
- âœ… Modify and create derivative works
- âœ… Distribute and sublicense

### You must:
- ğŸ“‹ Include the original license and copyright notice
- ğŸ“‹ State any significant changes made
- ğŸ“‹ Include a copy of the Apache 2.0 license

### Commercial Use

**Enterprise features** (MetaController, Advanced EDM, Real-time Evaluation) require a separate commercial license. See `COMMERCIAL_LICENSE.md` for details.

---

## ğŸŒŸ Citation

If you use HB-Eval System in your research, please cite:

```bibtex
@software{hb_eval_system_2025,
  title={HB-Eval System: Behavioral Evaluation \& Trustworthy Agentic AI},
  author={Abuelgasim Mohamed Ibrahim Adam},
  year={2025},
  url={https://github.com/hb-evalSystem/HB-System},
  version={1.0.0},
  license={Apache-2.0}
}
```

---

## ğŸ“§ Contact & Support

- **Email**: hbevalframe@gmail.com
- **GitHub Issues**: [Report bugs or request features](https://github.com/hb-evalSystem/HB-System/issues)
- **Discussions**: [Community discussions](https://github.com/hb-evalSystem/HB-System/discussions)

---

## ğŸ™ Acknowledgments

Special thanks to the research community for valuable feedback and to all contributors who help improve this framework.

---

## ğŸ”— Links

- ğŸŒ **Repository**: https://github.com/hb-evalSystem/HB-System
- ğŸ“š **Documentation**: Coming soon
- ğŸ¢ **Company Website**: Under development
- ğŸ“„ **Research Papers**: To be published in 2025

---

<p align="center">
  <b>Made with â¤ï¸ for the AI Research Community</b>
</p>

<p align="center">
  <i>Â© 2025 Abuelgasim Mohamed Ibrahim Adam. All rights reserved.</i>
</p>
